{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Czg1rhNykikI"
   },
   "source": [
    "# Training a ConvNet PyTorch\n",
    "\n",
    "In this notebook, you'll learn how to use the powerful PyTorch framework to specify a conv net architecture and train it on the human action recognition dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDLBxflrkikP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,sampler,Dataset\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import timeit\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QElCZx6Lkikk"
   },
   "source": [
    "## What's this PyTorch business?\n",
    "\n",
    "* When using a framework like PyTorch or TensorFlow you can harness the power of the GPU for your own custom neural network architectures without having to write CUDA code directly.\n",
    "* this notebook will walk you through much of what you need to do to train models using pytorch. if you want to learn more or need further clarification on topics that aren't fully explained here, here are 2 good Pytorch tutorials. 1): http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html 2)http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "* It's not necessary to have a GPU for this homework, using a GPU can make your code run faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9E8N5CS8kikp"
   },
   "source": [
    "## Load Datasets\n",
    "\n",
    "In this part, we will load the action recognition dataset for the neural network. In order to load data from our custom dataset, we need to write a custom Dataloader. If you put q3_2_data.mat, /valClips,/trainClips,/testClips under the folder of ./data/ , you do not need to change anything in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9D-QCMmzkikv"
   },
   "source": [
    "First, load the labels of the dataset, you should write your path of the q3_2_data.mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruJKe2KPkik3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7770\n",
      "2230\n"
     ]
    }
   ],
   "source": [
    "label_mat=scipy.io.loadmat('./data/q3_2_data.mat')\n",
    "label_train=label_mat['trLb']\n",
    "print(len(label_train))\n",
    "label_val=label_mat['valLb']\n",
    "print(len(label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O7YeCsVGkilG"
   },
   "source": [
    "### Dataset class\n",
    "\n",
    "torch.utils.data.Dataset is an abstract class representing a dataset. The custom dataset should inherit Dataset and override the following methods:\n",
    "\n",
    "    __len__ so that len(dataset) returns the size of the dataset.\n",
    "    __getitem__ to support the indexing such that dataset[i] can be used to get ith sample\n",
    "\n",
    "Letâ€™s create a dataset class for our action recognition dataset. We will read images in __getitem__. This is memory efficient because all the images are not stored in the memory at once but read as required.\n",
    "\n",
    "Sample of our dataset will be a dict {'image':image,'img_path':img_path,'Label':Label}. Our datset will take an optional argument transform so that any required processing can be applied on the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gwoCMo8XkilH"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ActionDataset(Dataset):\n",
    "    \"\"\"Action dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,  root_dir,labels=[], transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            labels(list): labels if images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.length=len(os.listdir(self.root_dir))\n",
    "        self.labels=labels\n",
    "    def __len__(self):\n",
    "        return self.length*3\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        folder=int(idx/3)+1\n",
    "        imidx=idx%3+1\n",
    "        folder=format(folder,'05d')\n",
    "        imgname=str(imidx)+'.jpg'\n",
    "        img_path = os.path.join(self.root_dir,\n",
    "                                folder,imgname)\n",
    "        image = Image.open(img_path)\n",
    "        if len(self.labels)!=0:\n",
    "            Label=self.labels[int(idx/3)][0]-1\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if len(self.labels)!=0:\n",
    "            sample={'image':image,'img_path':img_path,'Label':Label}\n",
    "        else:\n",
    "            sample={'image':image,'img_path':img_path}\n",
    "        return sample\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWuBpSGfkilQ"
   },
   "source": [
    "Iterating over the dataset by a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpN7YzC1kilR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00001\\1.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00001\\2.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00001\\3.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00002\\1.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00002\\2.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00002\\3.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00003\\1.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00003\\2.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00003\\3.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00004\\1.jpg\n"
     ]
    }
   ],
   "source": [
    "image_dataset=ActionDataset(root_dir='./data/trainClips/',\\\n",
    "                            labels=label_train,transform=T.ToTensor())\n",
    "\n",
    "#iterating though the dataset\n",
    "for i in range(10):\n",
    "    sample=image_dataset[i]\n",
    "    print(sample['image'].shape)\n",
    "    print(sample['Label'])\n",
    "    print(sample['img_path'])\n",
    "     \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cg4HxucHkilW"
   },
   "source": [
    "We can iterate over the created dataset with a 'for' loop as before. However, we are losing a lot of features by using a simple for loop to iterate over the data. In particular, we are missing out on:\n",
    "\n",
    "* Batching the data\n",
    "* Shuffling the data\n",
    "* Load the data in parallel using multiprocessing workers.\n",
    "\n",
    "torch.utils.data.DataLoader is an iterator which provides all these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_kyIRhJkilY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04715\\\\2.jpg', './data/trainClips/06997\\\\2.jpg', './data/trainClips/01324\\\\1.jpg', './data/trainClips/04755\\\\2.jpg'] tensor([5., 8., 1., 5.])\n",
      "1 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04687\\\\1.jpg', './data/trainClips/04728\\\\2.jpg', './data/trainClips/02008\\\\3.jpg', './data/trainClips/05357\\\\2.jpg'] tensor([5., 5., 2., 6.])\n",
      "2 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04602\\\\2.jpg', './data/trainClips/02145\\\\1.jpg', './data/trainClips/01136\\\\1.jpg', './data/trainClips/06170\\\\2.jpg'] tensor([5., 2., 1., 7.])\n",
      "3 torch.Size([4, 3, 64, 64]) ['./data/trainClips/02396\\\\3.jpg', './data/trainClips/00154\\\\2.jpg', './data/trainClips/05192\\\\3.jpg', './data/trainClips/05440\\\\1.jpg'] tensor([2., 0., 6., 6.])\n",
      "4 torch.Size([4, 3, 64, 64]) ['./data/trainClips/01493\\\\3.jpg', './data/trainClips/04980\\\\3.jpg', './data/trainClips/01414\\\\1.jpg', './data/trainClips/07203\\\\3.jpg'] tensor([1., 6., 1., 9.])\n",
      "5 torch.Size([4, 3, 64, 64]) ['./data/trainClips/05372\\\\1.jpg', './data/trainClips/01556\\\\3.jpg', './data/trainClips/00555\\\\2.jpg', './data/trainClips/05992\\\\2.jpg'] tensor([6., 1., 0., 7.])\n",
      "6 torch.Size([4, 3, 64, 64]) ['./data/trainClips/07514\\\\1.jpg', './data/trainClips/01989\\\\2.jpg', './data/trainClips/01161\\\\3.jpg', './data/trainClips/00471\\\\1.jpg'] tensor([9., 2., 1., 0.])\n",
      "7 torch.Size([4, 3, 64, 64]) ['./data/trainClips/03672\\\\2.jpg', './data/trainClips/02909\\\\2.jpg', './data/trainClips/05152\\\\2.jpg', './data/trainClips/02687\\\\1.jpg'] tensor([4., 3., 6., 3.])\n",
      "8 torch.Size([4, 3, 64, 64]) ['./data/trainClips/03993\\\\2.jpg', './data/trainClips/02993\\\\2.jpg', './data/trainClips/04092\\\\1.jpg', './data/trainClips/00582\\\\1.jpg'] tensor([4., 3., 4., 0.])\n",
      "9 torch.Size([4, 3, 64, 64]) ['./data/trainClips/02361\\\\3.jpg', './data/trainClips/03858\\\\1.jpg', './data/trainClips/00979\\\\3.jpg', './data/trainClips/03214\\\\1.jpg'] tensor([2., 4., 1., 3.])\n",
      "10 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04841\\\\2.jpg', './data/trainClips/00284\\\\3.jpg', './data/trainClips/07454\\\\2.jpg', './data/trainClips/07243\\\\3.jpg'] tensor([5., 0., 9., 9.])\n",
      "11 torch.Size([4, 3, 64, 64]) ['./data/trainClips/03691\\\\1.jpg', './data/trainClips/04670\\\\3.jpg', './data/trainClips/01173\\\\1.jpg', './data/trainClips/06656\\\\3.jpg'] tensor([4., 5., 1., 8.])\n",
      "12 torch.Size([4, 3, 64, 64]) ['./data/trainClips/07564\\\\3.jpg', './data/trainClips/06166\\\\1.jpg', './data/trainClips/02619\\\\3.jpg', './data/trainClips/06199\\\\1.jpg'] tensor([9., 7., 2., 7.])\n",
      "13 torch.Size([4, 3, 64, 64]) ['./data/trainClips/06423\\\\2.jpg', './data/trainClips/00081\\\\2.jpg', './data/trainClips/01357\\\\1.jpg', './data/trainClips/03861\\\\1.jpg'] tensor([7., 0., 1., 4.])\n",
      "14 torch.Size([4, 3, 64, 64]) ['./data/trainClips/01225\\\\2.jpg', './data/trainClips/06596\\\\2.jpg', './data/trainClips/05968\\\\3.jpg', './data/trainClips/05778\\\\2.jpg'] tensor([1., 8., 7., 7.])\n",
      "15 torch.Size([4, 3, 64, 64]) ['./data/trainClips/05743\\\\1.jpg', './data/trainClips/00704\\\\3.jpg', './data/trainClips/03370\\\\1.jpg', './data/trainClips/01950\\\\1.jpg'] tensor([7., 0., 3., 2.])\n",
      "16 torch.Size([4, 3, 64, 64]) ['./data/trainClips/01314\\\\2.jpg', './data/trainClips/02781\\\\2.jpg', './data/trainClips/02821\\\\2.jpg', './data/trainClips/01724\\\\2.jpg'] tensor([1., 3., 3., 1.])\n",
      "17 torch.Size([4, 3, 64, 64]) ['./data/trainClips/03425\\\\3.jpg', './data/trainClips/03036\\\\2.jpg', './data/trainClips/01583\\\\2.jpg', './data/trainClips/02192\\\\1.jpg'] tensor([3., 3., 1., 2.])\n",
      "18 torch.Size([4, 3, 64, 64]) ['./data/trainClips/03238\\\\1.jpg', './data/trainClips/06332\\\\2.jpg', './data/trainClips/03852\\\\3.jpg', './data/trainClips/05103\\\\2.jpg'] tensor([3., 7., 4., 6.])\n",
      "19 torch.Size([4, 3, 64, 64]) ['./data/trainClips/03213\\\\3.jpg', './data/trainClips/07633\\\\3.jpg', './data/trainClips/01976\\\\2.jpg', './data/trainClips/00143\\\\2.jpg'] tensor([3., 9., 2., 0.])\n",
      "20 torch.Size([4, 3, 64, 64]) ['./data/trainClips/07431\\\\1.jpg', './data/trainClips/01025\\\\3.jpg', './data/trainClips/01580\\\\3.jpg', './data/trainClips/05785\\\\2.jpg'] tensor([9., 1., 1., 7.])\n",
      "21 torch.Size([4, 3, 64, 64]) ['./data/trainClips/06630\\\\3.jpg', './data/trainClips/02956\\\\2.jpg', './data/trainClips/04798\\\\3.jpg', './data/trainClips/02164\\\\2.jpg'] tensor([8., 3., 5., 2.])\n"
     ]
    }
   ],
   "source": [
    "image_dataloader = DataLoader(image_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "for i,sample in enumerate(image_dataloader):\n",
    "    sample['image']=sample['image'].cuda()\n",
    "    print(i,sample['image'].shape,sample['img_path'],sample['Label'])\n",
    "    if i>20: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWlY1-Tpkild"
   },
   "source": [
    "Dataloaders for the training, validationg and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88bh4JpKkile"
   },
   "outputs": [],
   "source": [
    "image_dataset_train=ActionDataset(root_dir='./data/trainClips/',labels=label_train,transform=T.ToTensor())\n",
    "\n",
    "image_dataloader_train = DataLoader(image_dataset_train, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "image_dataset_val=ActionDataset(root_dir='./data/valClips/',labels=label_val,transform=T.ToTensor())\n",
    "\n",
    "image_dataloader_val = DataLoader(image_dataset_val, batch_size=32,\n",
    "                        shuffle=False, num_workers=0)\n",
    "image_dataset_test=ActionDataset(root_dir='./data/testClips/',labels=[],transform=T.ToTensor())\n",
    "\n",
    "image_dataloader_test = DataLoader(image_dataset_test, batch_size=32,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NlHfd8Opkili"
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor # the CPU datatype\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5Vwl83Lkill"
   },
   "source": [
    "## Example Model\n",
    "\n",
    "### Some assorted tidbits\n",
    "\n",
    "Let's start by looking at a simple model. First, note that PyTorch operates on Tensors, which are n-dimensional arrays functionally analogous to numpy's ndarrays, with the additional feature that they can be used for computations on GPUs.\n",
    "\n",
    "We'll provide you with a Flatten function, which we explain here. Remember that our image data (and more relevantly, our intermediate feature maps) are initially N x C x H x W, where:\n",
    "* N is the number of datapoints\n",
    "* C is the number of image channels. \n",
    "* H is the height of the intermediate feature map in pixels\n",
    "* W is the height of the intermediate feature map in pixels\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that needs spatial understanding of where the intermediate features are relative to each other. When we input  data into fully connected affine layers, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"Flatten\" operation to collapse the C x H x W values per representation into a single long vector. The Flatten function below first reads in the N, C, H, and W values from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be C x H x W, but we don't need to specify that explicitly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfjCjTsnkilm"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0akDg9Zkilp"
   },
   "source": [
    "### The example model itself\n",
    "\n",
    "The first step to training your own model is defining its architecture.\n",
    "\n",
    "Here's an example of a convolutional neural network defined in PyTorch -- try to understand what each line is doing, remembering that each layer is composed upon the previous layer. We haven't trained anything yet - that'll come next - for now, we want you to understand how everything gets set up.  nn.Sequential is a container which applies each layer\n",
    "one after the other.\n",
    "\n",
    "In this example, you see 2D convolutional layers (Conv2d), ReLU activations, and fully-connected layers (Linear). You also see the Cross-Entropy loss function, and the Adam optimizer being used. \n",
    "\n",
    "Make sure you understand why the parameters of the Linear layer are 10092 and 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86Z0XTsZkilq"
   },
   "outputs": [],
   "source": [
    "# Here's where we define the architecture of the model... \n",
    "simple_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=7, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                Flatten(), # see above for explanation\n",
    "                nn.Linear(10092, 10), # affine layer\n",
    "              )\n",
    "\n",
    "# Set the type of all data in this model to be FloatTensor \n",
    "simple_model.type(dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(simple_model.parameters(), lr=1e-2) # lr sets the learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ewte7dUbkilt"
   },
   "source": [
    "PyTorch supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). \n",
    "\n",
    "* Layers: http://pytorch.org/docs/nn.html\n",
    "* Activations: http://pytorch.org/docs/nn.html#non-linear-activations\n",
    "* Loss functions: http://pytorch.org/docs/nn.html#loss-functions\n",
    "* Optimizers: http://pytorch.org/docs/optim.html#algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0l2cX4lkilu"
   },
   "source": [
    "## Training a specific model\n",
    "\n",
    "In this section, we're going to specify a model for you to construct. The goal here isn't to get good performance (that'll be next), but instead to get comfortable with understanding the PyTorch documentation and configuring your own model. \n",
    "\n",
    "Using the code provided above as guidance, and using the following PyTorch documentation, specify a model with the following architecture:\n",
    "\n",
    "* 7x7 Convolutional Layer with 8 filters and stride of 1\n",
    "* ReLU Activation Layer\n",
    "* 2x2 Max Pooling layer with a stride of 2\n",
    "* 7x7 Convolutional Layer with 16 filters and stride of 1\n",
    "* ReLU Activation Layer\n",
    "* 2x2 Max Pooling layer with a stride of 2\n",
    "* Flatten the feature map\n",
    "* ReLU Activation Layer\n",
    "* Affine layer to map input units to 10 outputs, you need to figure out the input size here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQHDD56Tkilv"
   },
   "outputs": [],
   "source": [
    "fixed_model_base = nn.Sequential( \n",
    "    #########1st TODO  (5 points)###################\n",
    "    nn.Conv2d(3, 8, kernel_size=7, stride=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(8, 16, kernel_size=7, stride=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    Flatten(),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16*11*11, 10)\n",
    "    ####################################\n",
    "            )\n",
    "fixed_model = fixed_model_base.type(dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6K1qjzb-kilx"
   },
   "source": [
    "To make sure you're doing the right thing, use the following tool to check the dimensionality of your output (it should be 32 x 10, since our batches have size 32 and the output of the final affine layer should be 10, corresponding to our 10 classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L029BLMHkily"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we're going to feed a random batch into the model you defined and make sure the output is the right size\n",
    "x = torch.randn(32, 3, 64, 64).type(dtype)\n",
    "x_var = Variable(x.type(dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model(x_var)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "print(np.array(ans.size()))\n",
    "np.array_equal(np.array(ans.size()), np.array([32, 10]))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dU4ZQyD2kil0"
   },
   "source": [
    "### Train the model.\n",
    "\n",
    "Now that you've seen how to define a model and do a single forward pass of some data through it, let's  walk through how you'd actually train one whole epoch over your training data (using the fixed_model_base we provided above).\n",
    "\n",
    "Make sure you understand how each PyTorch function used below corresponds to what you implemented in your custom neural network implementation.\n",
    "\n",
    "Note that because we are not resetting the weights anywhere below, if you run the cell multiple times, you are effectively training multiple epochs (so your performance should improve).\n",
    "\n",
    "First, set up an RMSprop optimizer (using a 1e-4 learning rate) and a cross-entropy loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CL0kHO_Rkil1"
   },
   "outputs": [],
   "source": [
    "################ 2nd TODO  (5 points)##################\n",
    "optimizer = torch.optim.RMSprop(fixed_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swBi5_QJkil3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 100, loss = 1.7901\n",
      "t = 200, loss = 1.4626\n",
      "t = 300, loss = 1.4024\n",
      "t = 400, loss = 1.4161\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-ce429b32aec7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load one batch at a time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_dataloader_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mx_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#print(type(x_var.data))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\karan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\karan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\karan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c1173a00001a>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m         img_path = os.path.join(self.root_dir,\n\u001b[0;32m     26\u001b[0m                                 folder,imgname)\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mLabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\karan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2777\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2779\u001b[1;33m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2781\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This sets the model in \"training\" mode. \n",
    "# This is relevant for some layers that may have different behavior\n",
    "# in training mode vs testing mode, such as Dropout and BatchNorm. \n",
    "fixed_model.train()\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, sample in enumerate(image_dataloader_train):\n",
    "    x_var = Variable(sample['image'])\n",
    "    #print(type(x_var.data))\n",
    "    #print(x_var.shape)\n",
    "    y_var = Variable(sample['Label']).long()\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = fixed_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.item()))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F39GZuPmkimH"
   },
   "source": [
    "Now you've seen how the training process works in PyTorch. To save you writing boilerplate code, we're providing the following helper functions to help you train for multiple epochs and check the accuracy of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkwbyfXwkimJ"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataloader, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, sample in enumerate(dataloader):\n",
    "            x_var = Variable(sample['image'])\n",
    "            y_var = Variable(sample['Label'].long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.item()))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['image'])\n",
    "        y_var = sample['Label']\n",
    "        #y_var=y_var.cpu()\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.max(1)#scores.data.cpu().max(1)\n",
    "        #print(preds)\n",
    "        #print(y_var)\n",
    "        num_correct += (preds.numpy() == y_var.numpy()).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgazBdowkimU"
   },
   "source": [
    "### Check the accuracy of the model.\n",
    "\n",
    "Let's see the train and check_accuracy code in action -- feel free to use these methods when evaluating the models you develop below.\n",
    "\n",
    "You should get a training loss of around 1.0-1.2, and a validation accuracy of around 50-60%. As mentioned above, if you re-run the cells, you'll be training more epochs, so your performance will improve past these numbers.\n",
    "\n",
    "But don't worry about getting these numbers better -- this was just practice before you tackle designing your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0KVG7cXkimc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 1\n",
      "t = 100, loss = 2.2570\n",
      "t = 200, loss = 2.0664\n",
      "t = 300, loss = 1.8087\n",
      "t = 400, loss = 1.5577\n",
      "t = 500, loss = 1.4869\n",
      "t = 600, loss = 1.3653\n",
      "t = 700, loss = 1.3690\n",
      "Got 12940 / 23310 correct (55.51)\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(12345)\n",
    "fixed_model.cpu()\n",
    "fixed_model.apply(reset) \n",
    "fixed_model.train() \n",
    "train(fixed_model, loss_fn, optimizer,image_dataloader_train, num_epochs=1) \n",
    "check_accuracy(fixed_model, image_dataloader_train)# check accuracy on the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUkUV3uFkimv"
   },
   "source": [
    "### Don't forget the validation set!\n",
    "\n",
    "And note that you can use the check_accuracy function to evaluate on the validation set, by passing **image_dataloader_val** as the second argument to check_accuracy. The accuracy on validation set is arround 40-50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIPA6RP8kimw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3142 / 6690 correct (46.97)\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(fixed_model, image_dataloader_val)#check accuracy on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sR_FF3Tkimz"
   },
   "source": [
    "##### Train a better  model for action recognition!\n",
    "\n",
    "Now it's your job to experiment with architectures, hyperparameters, loss functions, and optimizers to train a model that achieves better accuracy on the action recognition **validation** set. You can use the check_accuracy and train functions from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6widA8Zkim2"
   },
   "source": [
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use Dropout.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at least 55% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "def check_accuracy_2d(model, loader, pretrained=None):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    for t, sample in enumerate(loader):\n",
    "        if t+1 % 1100==0:\n",
    "            print('wait')\n",
    "        x_var = Variable(sample['image'].type(dtype))\n",
    "        y_var = sample['Label'].type(dtype)\n",
    "        model = model.type(dtype)\n",
    "        y_var=y_var.cpu()\n",
    "        if pretrained:\n",
    "            pretrained = pretrained.type(dtype)\n",
    "            x_var = pretrained(x_var)\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds.numpy() == y_var.numpy()).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc * 100\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.Resize(224),\n",
    "#     T.RandomOrder([\n",
    "#         T.RandomVerticalFlip(p=0.8),\n",
    "#         T.RandomRotation(45),\n",
    "#         T.RandomPerspective()\n",
    "#     ]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_tf = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "image_dataset_train=ActionDataset(root_dir='./data/trainClips/',labels=label_train,transform=train_tf)\n",
    "\n",
    "image_dataloader_train = DataLoader(image_dataset_train, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "image_dataset_val=ActionDataset(root_dir='./data/valClips/',labels=label_val,transform=train_tf)\n",
    "\n",
    "image_dataloader_val = DataLoader(image_dataset_val, batch_size=32,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkBshPcykim3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c3061b2828>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 =>  Val acc: 88.89387144992527\n",
      "Got 5947 / 6690 correct (88.89)\n"
     ]
    }
   ],
   "source": [
    "###########3rd TODO (6 points, must submit the results to Kaggle) ##############\n",
    "# Train your model here, and make sure the output of this cell is the accuracy of your best model on the \n",
    "# train, val, and test sets. Here's some code to get you started. The output of this cell should be the training\n",
    "# and validation accuracy on your best model (measured by validation accuracy).\n",
    "%matplotlib inline \n",
    "import torchvision.models as models\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pretrained = models.resnet18(pretrained=True)\n",
    "pretrained.eval()\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1000, 1000),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(1000, 10)\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-4)\n",
    "\n",
    "# train(fixed_model_base, loss_fn, optimizer,image_dataloader_train, num_epochs=1) \n",
    "\n",
    "# training\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_acc = 60\n",
    "PATH = 'saved_models/2d_{}.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    for t, sample in enumerate(image_dataloader_train):\n",
    "        model.train()\n",
    "        x_var = sample['image'].type(dtype)\n",
    "        y_var = sample['Label'].type(dtype).long()\n",
    "        \n",
    "        model = model.type(dtype)\n",
    "        if pretrained:\n",
    "            pretrained = pretrained.type(dtype)\n",
    "            x_var = pretrained(x_var)\n",
    "        scores = model(x_var)\n",
    "\n",
    "        loss = loss_fn(scores.cpu(), y_var.cpu())\n",
    "        if (t + 1) % 100 == 0:\n",
    "            print('t = %d, loss = %.4f' % (t + 1, loss.data.item()))\n",
    "        losses.append(loss.data.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "    train_losses.append(np.mean(np.array(losses)))\n",
    "    \n",
    "    del x_var, y_var, scores\n",
    "    acc = check_accuracy_2d(model, image_dataloader_val, pretrained=pretrained)\n",
    "\n",
    "    val_losses.append(np.mean(np.array(losses)))\n",
    "    \n",
    "    val_accs.append(acc)\n",
    "    if epoch > 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(221)\n",
    "        plt.plot(train_losses)\n",
    "        plt.subplot(222)\n",
    "        plt.plot(val_accs)\n",
    "        plt.show()\n",
    "        print(\"Epoch\", epoch+1, \"=> \", \"Val acc:\", acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model, PATH.format(str(acc)[:5]))\n",
    "\n",
    "fixed_model = torch.load(PATH.format(str(best_acc)[:5]))\n",
    "best_acc = check_accuracy_2d(fixed_model, image_dataloader_val, pretrained = pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jsq5u4ZXkim6"
   },
   "source": [
    "### Describe what you did \n",
    "\n",
    "In the cell below you should write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BorliZPkim6"
   },
   "source": [
    "Tell us here!\n",
    "###########4th TODO (4 points) ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have run experiments for this part in different notebook named '**image.ipynb**'. Please have a look at it for rest of the description. <br>\n",
    "**Shorter Version** <br>\n",
    "1)Baseline model(the given one): Val acc: 73.44, kaggle score:65.88 <br>\n",
    "2)Pretrained Resnet18 (without augmentation) Val acc: 82, Kaggle score: 84.335 <br>\n",
    "3)Pretrained Resnet18 (with augmentation) + 2 trained layers from above. Val acc: 79.19 <br>\n",
    "4)Pretrained Resnet18 (with augmentation) + 2 trainable layers from scratch. Val acc: 79.52 <br>\n",
    "5)Pretrained Resnet101 (without augmentation) Val acc: 86.30, kaggle score: **85.898** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OEGCO0mGkim7"
   },
   "source": [
    "### Testing the model and submit on Kaggle\n",
    "Testing the model on the testing set and save the results as a .csv file. \n",
    "Please submitted the results.csv file generated by predict_on_test() to Kaggle(https://www.kaggle.com/c/cse512springhw3) to see how well your network performs on the test set. \n",
    "#######5th TODO (submit the result to Kaggle, the highest 3 entries get extra 10 points )###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fG49zJzdkim8"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_mm",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-463f9c917780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_on_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfixed_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_dataloader_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-463f9c917780>\u001b[0m in \u001b[0;36mpredict_on_test\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\karan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\karan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\karan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\karan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\karan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_mm"
     ]
    }
   ],
   "source": [
    "def predict_on_test(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    results=open('results.csv','w')\n",
    "    count=0\n",
    "    results.write('Id'+','+'Class'+'\\n')\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['image'])\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.max(1)\n",
    "        for i in range(len(preds)):\n",
    "            results.write(str(count)+','+str(preds[i])+'\\n')\n",
    "            count+=1\n",
    "    results.close()\n",
    "    return count\n",
    "    \n",
    "count=predict_on_test(fixed_model, image_dataloader_test)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Implemented Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9810\n"
     ]
    }
   ],
   "source": [
    "def predict_on_test2d(model, loader, pretrained = None):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    results=open('results.csv','w')\n",
    "    count=0\n",
    "    results.write('Id'+','+'Class'+'\\n')\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['image']).type(dtype)\n",
    "        model = model.type(dtype)\n",
    "        if pretrained:\n",
    "            pretrained = pretrained.type(dtype)\n",
    "            x_var = pretrained(x_var)\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.max(1)\n",
    "        for i in range(len(preds)):\n",
    "            results.write(str(count)+','+str(preds[i].item())+'\\n')\n",
    "            count+=1\n",
    "    results.close()\n",
    "    return count\n",
    "\n",
    "count=predict_on_test2d(fixed_model, image_dataloader_test, pretrained=pretrained)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gojKr6H0kinD"
   },
   "source": [
    "### GPU! (This part is optional, 0 points)\n",
    "\n",
    "If you have access to GPU, you can make the code run on GPU, it would be much faster. \n",
    "\n",
    "Now, we're going to switch the dtype of the model and our data to the GPU-friendly tensors, and see what happens... everything is the same, except we are casting our model and input tensors as this new dtype instead of the old one.\n",
    "\n",
    "If this returns false, or otherwise fails in a not-graceful way (i.e., with some error message), you may not have an NVIDIA GPU available on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nx_8-bDUkinE"
   },
   "outputs": [],
   "source": [
    "# Verify that CUDA is properly configured and you have a GPU available\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qd19oDZzkinG"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "fixed_model_gpu = copy.deepcopy(fixed_model_base)#.type(gpu_dtype)\n",
    "fixed_model_gpu.cuda()\n",
    "x_gpu = torch.randn(4, 3, 64, 64).cuda()#.type(gpu_dtype)\n",
    "x_var_gpu = Variable(x_gpu)#type(gpu_dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([4, 10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4gsqJpakinJ"
   },
   "source": [
    "Run the following cell to evaluate the performance of the forward pass running on the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M9RH8SlYkinK"
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "ans = fixed_model(x_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mS9ZHu5ikinM"
   },
   "source": [
    "... and now the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CXo7XPCykinN"
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations\n",
    "ans = fixed_model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pHvTgcdykinQ"
   },
   "source": [
    "You should observe that even a simple forward pass like this is significantly faster on the GPU. So for the rest of the assignment (and when you go train your models in assignment 3 and your project!), you should use the GPU datatype for your model and your tensors: as a reminder that is *torch.cuda.FloatTensor* (in our notebook here as *gpu_dtype*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCVe55-wkinR"
   },
   "source": [
    "Let's make the loss function and training variables to GPU friendly format by '.cuda()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "A6cZOqNhkinS"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.RMSprop(fixed_model_gpu.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xrW8_ZHCkinU"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataloader, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, sample in enumerate(dataloader):\n",
    "            x_var = Variable(sample['image'].cuda())\n",
    "            y_var = Variable(sample['Label'].cuda().long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['image'].cuda())\n",
    "        y_var = sample['Label'].cuda()\n",
    "        y_var=y_var.cpu()\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        #print(preds)\n",
    "        #print(y_var)\n",
    "        num_correct += (preds.numpy() == y_var.numpy()).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4iqg6gwCkinW"
   },
   "source": [
    "Run on GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4XU96uDgkinX"
   },
   "outputs": [],
   "source": [
    "torch.cuda.random.manual_seed(12345)\n",
    "\n",
    "fixed_model_gpu.apply(reset) \n",
    "fixed_model_gpu.train() \n",
    "train(fixed_model_gpu, loss_fn, optimizer,image_dataloader_train, num_epochs=1) \n",
    "check_accuracy(fixed_model_gpu, image_dataloader_train)# check accuracy on the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jC4jmL_Fkina"
   },
   "source": [
    "### 3D Convolution on video clips (15 points+10 extra points)\n",
    "3D convolution is for videos, it has one more dimension than 2d convolution. You can find the document for 3D convolution here http://pytorch.org/docs/master/nn.html#torch.nn.Conv3dIn. In our dataset, each clip is a video of 3 frames. Lets classify the each clip rather than each image using 3D convolution.\n",
    "We offer the data loader, the train_3d and check_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Wllmp4Nkinb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00001\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00002\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00003\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00004\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00005\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00006\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00007\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00008\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00009\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00010\n"
     ]
    }
   ],
   "source": [
    "class ActionClipDataset(Dataset):\n",
    "    \"\"\"Action Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,  root_dir,labels=[], transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.length=len(os.listdir(self.root_dir))\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        folder=idx+1\n",
    "        folder=format(folder,'05d')\n",
    "        clip=[]\n",
    "        if len(self.labels)!=0:\n",
    "            Label=self.labels[idx][0]-1\n",
    "        for i in range(3):\n",
    "            imidx=i+1\n",
    "            imgname=str(imidx)+'.jpg'\n",
    "            img_path = os.path.join(self.root_dir,\n",
    "                                    folder,imgname)\n",
    "            image = Image.open(img_path)\n",
    "            image=np.array(image)\n",
    "            clip.append(image)\n",
    "        if self.transform:\n",
    "            clip=np.asarray(clip)\n",
    "            clip=np.transpose(clip, (0,3,1,2))\n",
    "            clip = torch.from_numpy(np.asarray(clip))\n",
    "        if len(self.labels)!=0:\n",
    "            sample={'clip':clip,'Label':Label,'folder':folder}\n",
    "        else:\n",
    "            sample={'clip':clip,'folder':folder}\n",
    "        return sample\n",
    "\n",
    "clip_dataset=ActionClipDataset(root_dir='./data/trainClips/',\\\n",
    "                               labels=label_train,transform=T.ToTensor())#/home/tqvinh/Study/CSE512/cse512-s18/hw2data/trainClips/\n",
    "for i in range(10):\n",
    "    sample=clip_dataset[i]\n",
    "    print(sample['clip'].shape)\n",
    "    print(sample['Label'])\n",
    "    print(sample['folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJBf8xpOkind"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 3, 3, 64, 64]) ['00366', '05452', '07511', '02337'] tensor([0., 6., 9., 2.])\n",
      "1 torch.Size([4, 3, 3, 64, 64]) ['03160', '00351', '02188', '02669'] tensor([3., 0., 2., 2.])\n",
      "2 torch.Size([4, 3, 3, 64, 64]) ['03272', '06322', '06785', '00676'] tensor([3., 7., 8., 0.])\n",
      "3 torch.Size([4, 3, 3, 64, 64]) ['02461', '01237', '03099', '06111'] tensor([2., 1., 3., 7.])\n",
      "4 torch.Size([4, 3, 3, 64, 64]) ['06781', '04129', '01642', '03866'] tensor([8., 4., 1., 4.])\n",
      "5 torch.Size([4, 3, 3, 64, 64]) ['00215', '00253', '06110', '02886'] tensor([0., 0., 7., 3.])\n",
      "6 torch.Size([4, 3, 3, 64, 64]) ['07169', '02253', '02633', '07367'] tensor([9., 2., 2., 9.])\n",
      "7 torch.Size([4, 3, 3, 64, 64]) ['06284', '07463', '03700', '01931'] tensor([7., 9., 4., 2.])\n",
      "8 torch.Size([4, 3, 3, 64, 64]) ['06107', '03446', '00926', '06027'] tensor([7., 3., 1., 7.])\n",
      "9 torch.Size([4, 3, 3, 64, 64]) ['00113', '06039', '05723', '00902'] tensor([0., 7., 7., 0.])\n",
      "10 torch.Size([4, 3, 3, 64, 64]) ['00496', '06244', '03938', '01214'] tensor([0., 7., 4., 1.])\n",
      "11 torch.Size([4, 3, 3, 64, 64]) ['03353', '04298', '05116', '02291'] tensor([3., 5., 6., 2.])\n",
      "12 torch.Size([4, 3, 3, 64, 64]) ['06764', '00377', '06691', '04681'] tensor([8., 0., 8., 5.])\n",
      "13 torch.Size([4, 3, 3, 64, 64]) ['02426', '00809', '05842', '03244'] tensor([2., 0., 7., 3.])\n",
      "14 torch.Size([4, 3, 3, 64, 64]) ['00711', '04061', '06010', '03861'] tensor([0., 4., 7., 4.])\n",
      "15 torch.Size([4, 3, 3, 64, 64]) ['00559', '06134', '04873', '04927'] tensor([0., 7., 5., 5.])\n",
      "16 torch.Size([4, 3, 3, 64, 64]) ['02344', '06609', '06294', '01964'] tensor([2., 8., 7., 2.])\n",
      "17 torch.Size([4, 3, 3, 64, 64]) ['01813', '03759', '00821', '03234'] tensor([1., 4., 0., 3.])\n",
      "18 torch.Size([4, 3, 3, 64, 64]) ['05505', '06277', '00763', '00424'] tensor([6., 7., 0., 0.])\n",
      "19 torch.Size([4, 3, 3, 64, 64]) ['05768', '02795', '00355', '00178'] tensor([7., 3., 0., 0.])\n",
      "20 torch.Size([4, 3, 3, 64, 64]) ['03437', '01794', '04209', '00163'] tensor([3., 1., 5., 0.])\n",
      "21 torch.Size([4, 3, 3, 64, 64]) ['06908', '06133', '04654', '04321'] tensor([8., 7., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "clip_dataloader = DataLoader(clip_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "for i,sample in enumerate(clip_dataloader):\n",
    "    print(i,sample['clip'].shape,sample['folder'],sample['Label'])\n",
    "    if i>20: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SylcAPWkinh"
   },
   "outputs": [],
   "source": [
    "clip_dataset_train=ActionClipDataset(root_dir='./data/trainClips/',labels=label_train,transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_train = DataLoader(clip_dataset_train, batch_size=16,\n",
    "                        shuffle=True, num_workers=0)\n",
    "clip_dataset_val=ActionClipDataset(root_dir='./data/valClips/',labels=label_val,transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_val = DataLoader(clip_dataset_val, batch_size=16,\n",
    "                        shuffle=True, num_workers=0)\n",
    "clip_dataset_test=ActionClipDataset(root_dir='./data/testClips/',labels=[],transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_test = DataLoader(clip_dataset_test, batch_size=16,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-p7djQlkinl"
   },
   "source": [
    "Write the Flatten for 3d covolution feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QONO_BMkinm"
   },
   "outputs": [],
   "source": [
    "class Flatten3d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        ###############6th TODO (3 points)###################\n",
    "        return x.view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wKlUOJwhkino"
   },
   "source": [
    "Design a network using 3D convolution on videos for video classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6lZTN5Ckinp"
   },
   "outputs": [],
   "source": [
    "fixed_model_3d = nn.Sequential( # You fill this in!\n",
    "    ###############7th TODO (8 points)#########################\n",
    "    \n",
    "    nn.Conv3d(3, 16, kernel_size=(2, 3, 3), padding=1),\n",
    "    nn.BatchNorm3d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2), padding=1),\n",
    "    \n",
    "    nn.Conv3d(16, 32, kernel_size=(2, 3, 3), padding=1),\n",
    "    nn.BatchNorm3d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2), padding=1),\n",
    "    \n",
    "    nn.Conv3d(32, 64, kernel_size=(2, 3, 3), padding=0),\n",
    "    nn.BatchNorm3d(64),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Conv3d(64, 64, kernel_size=(2, 3, 3), padding=1),\n",
    "    nn.BatchNorm3d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2), padding=1),\n",
    "    \n",
    "    Flatten3d(),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(32768),\n",
    "    nn.Linear(32768, 10)    \n",
    ")\n",
    "\n",
    "fixed_model_3d = fixed_model_3d.type(dtype)\n",
    "x = torch.randn(32,3, 3, 64, 64).type(dtype)\n",
    "x_var = Variable(x).type(dtype) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model_3d(x_var) \n",
    "np.array_equal(np.array(ans.size()), np.array([32, 10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF0h635Rkins"
   },
   "source": [
    "### Describe what you did (4 points)\n",
    "\n",
    "In the cell below you should write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dc0X3Cmkint"
   },
   "source": [
    "### 8th TODO Tell us here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major part of 'what I did' for this section is explained in different notebook file named '**video.ipynb**'. Please have a look at it for rest of description. <br>\n",
    "**Short Version** <br>\n",
    "1)Basic model: Val acc: 70.49 Kaggle score: 66.055 <br>\n",
    "2)Basic model (with augmentation): Val acc: 70.26 Kaggle score: 67.380 <br>\n",
    "2)Pretrained Resnet18-3d (without augmentation) Val acc: 86 Kaggle Score: 81.039 <br>\n",
    "3)Pretrained Resnet18-3d (with augmentation) Val acc: 87.71 Kaggle Score: **82.059** <br>\n",
    "4)Pretrained Resnet18-3d (with augmentation, minor changes in parameters) Val acc: 85.20 Kaggle Score: 80.937 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dnJsH-Fokinu"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.RMSprop(fixed_model_3d.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Bd2qla8kinw"
   },
   "outputs": [],
   "source": [
    "def train_3d(model, loss_fn, optimizer,dataloader,num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, sample in enumerate(dataloader):\n",
    "            x_var = Variable(sample['clip'].type(dtype))\n",
    "            y_var = Variable(sample['Label'].type(dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def check_accuracy_3d(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['clip'].type(dtype))\n",
    "        y_var = sample['Label'].type(dtype)\n",
    "        y_var=y_var.cpu()\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        #print(preds)\n",
    "        #print(y_var)\n",
    "        num_correct += (preds.numpy() == y_var.numpy()).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9K2IQkIkiny"
   },
   "outputs": [],
   "source": [
    "torch.cuda.random.manual_seed(12345)\n",
    "fixed_model_3d.apply(reset) \n",
    "fixed_model_3d.train() \n",
    "train_3d(fixed_model_3d, loss_fn, optimizer,clip_dataloader_train, num_epochs=1) \n",
    "fixed_model_3d.eval() \n",
    "check_accuracy_3d(fixed_model_3d, clip_dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_3d(model, loader, pretrained=None):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    for t, sample in enumerate(loader):\n",
    "        if t+1 % 1100==0:\n",
    "            print('wait')\n",
    "        x_var = Variable(sample['clip'].type(dtype))\n",
    "        y_var = sample['Label'].type(dtype)\n",
    "        model = model.type(dtype)\n",
    "        y_var=y_var.cpu()\n",
    "        if pretrained:\n",
    "            pretrained = pretrained.type(dtype)\n",
    "            x_var = pretrained(x_var)\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds.numpy() == y_var.numpy()).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c30632f208>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 =>  Val acc: 65.7847533632287\n"
     ]
    }
   ],
   "source": [
    "train_tf = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomOrder([\n",
    "        T.RandomChoice([\n",
    "            T.RandomHorizontalFlip(p=1),\n",
    "            T.RandomVerticalFlip(p=1),\n",
    "        ]),\n",
    "        T.RandomRotation(90),\n",
    "        T.RandomErasing(p = 1, value = 'random'),\n",
    "        T.RandomPerspective()\n",
    "    ])\n",
    "    \n",
    "])\n",
    "clip_dataset_train=ActionClipDataset(root_dir='./data/trainClips/',labels=label_train,transform=train_tf)\n",
    "\n",
    "clip_dataloader_train = DataLoader(clip_dataset_train, batch_size=8,\n",
    "                        shuffle=True, num_workers=0)\n",
    "clip_dataset_val=ActionClipDataset(root_dir='./data/valClips/',labels=label_val,transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_val = DataLoader(clip_dataset_val, batch_size=8,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "model = fixed_model_3d\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "pretrained = None\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_acc = 50\n",
    "PATH = 'saved_models/3d_{}.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    for t, sample in enumerate(clip_dataloader_train):    \n",
    "        model.train()\n",
    "\n",
    "        x_var = sample['clip'].type(dtype)\n",
    "        y_var = sample['Label'].type(dtype).long()\n",
    "        \n",
    "        \n",
    "        model = model.type(dtype)\n",
    "        \n",
    "        if pretrained:\n",
    "            pretrained = pretrained.type(dtype)\n",
    "            x_var = pretrained(x_var)\n",
    "        scores = model(x_var)\n",
    "\n",
    "        loss = loss_fn(scores.cpu(), y_var.cpu())\n",
    "        if (t + 1) % 200 == 0:\n",
    "            print('t = %d, loss = %.4f' % (t + 1, loss.data.item()))\n",
    "        losses.append(loss.data.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "    train_losses.append(np.mean(np.array(losses)))\n",
    "    acc = check_accuracy_3d(model, clip_dataloader_val, pretrained=pretrained)\n",
    "\n",
    "    val_losses.append(np.mean(np.array(losses)))\n",
    "    \n",
    "    val_accs.append(acc)\n",
    "    if epoch > 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(221)\n",
    "        plt.plot(train_losses)\n",
    "        plt.subplot(222)\n",
    "        plt.plot(val_accs)\n",
    "        plt.show()\n",
    "        print(\"Epoch\", epoch+1, \"=> \", \"Val acc:\", acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model, PATH.format(str(acc)[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1525 / 2230 correct (68.39)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68.38565022421524"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_model_3d = torch.load(PATH.format(str(best_acc)[:5]))\n",
    "fixed_model_3d.eval() \n",
    "check_accuracy_3d(fixed_model_3d, clip_dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next experiment is done in different notebook file, here I have shared the code and his val accuracy using saved model. Please find the experiment in the file named '**video.ipynb**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = torchvision.models.video.r3d_18(pretrained=True)\n",
    "model = pretrained\n",
    "model.fc = nn.Linear(512,10)\n",
    "pretrained = None\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum = 0.8)\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.Resize(112),\n",
    "    T.ToTensor(),\n",
    "    T.RandomOrder([\n",
    "        T.RandomVerticalFlip(p=0.8)\n",
    "    ]),\n",
    "    T.Normalize(mean = [0.43216, 0.394666, 0.37645], std = [0.22803, 0.22145, 0.216989])\n",
    "])\n",
    "val_tf = T.Compose([\n",
    "    T.Resize(112),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean = [0.43216, 0.394666, 0.37645], std = [0.22803, 0.22145, 0.216989])\n",
    "])\n",
    "\n",
    "clip_dataset_train=ActionClipDataset(root_dir='./data/trainClips/',labels=label_train,transform=train_tf)\n",
    "\n",
    "clip_dataloader_train = DataLoader(clip_dataset_train, batch_size=10,\n",
    "                        shuffle=True, num_workers=0)\n",
    "clip_dataset_val=ActionClipDataset(root_dir='./data/valClips/',labels=label_val,transform=val_tf)\n",
    "\n",
    "clip_dataloader_val = DataLoader(clip_dataset_val, batch_size=10,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "num_epochs = 15\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_acc = 87.71\n",
    "PATH = 'saved_models/Norm3d_{}.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    for t, sample in enumerate(clip_dataloader_train):    \n",
    "        model.train()\n",
    "\n",
    "        x_var = sample['clip'].type(dtype)\n",
    "        y_var = sample['Label'].type(dtype).long()\n",
    "        \n",
    "        \n",
    "        model = model.type(dtype)\n",
    "        \n",
    "        if pretrained:\n",
    "            pretrained = pretrained.type(dtype)\n",
    "            x_var = pretrained(x_var)\n",
    "        scores = model(x_var)\n",
    "\n",
    "        loss = loss_fn(scores.cpu(), y_var.cpu())\n",
    "        if (t + 1) % 200 == 0:\n",
    "            print('t = %d, loss = %.4f' % (t + 1, loss.data.item()))\n",
    "        losses.append(loss.data.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "        optimizer.step()\n",
    "    train_losses.append(np.mean(np.array(losses)))\n",
    "    acc = check_accuracy_3d(model, clip_dataloader_val, pretrained=pretrained)\n",
    "\n",
    "    val_losses.append(np.mean(np.array(losses)))\n",
    "    \n",
    "    val_accs.append(acc)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model, PATH.format(str(acc)[:5]))\n",
    "        \n",
    "    if epoch > 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(221)\n",
    "        plt.plot(train_losses)\n",
    "        plt.subplot(222)\n",
    "        plt.plot(val_accs)\n",
    "        plt.show()\n",
    "        print(\"Epoch\", epoch+1, \"=> \", \"Val acc:\", acc, \"=> Best Till now\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1956 / 2230 correct (87.71)\n"
     ]
    }
   ],
   "source": [
    "# model experiment is there in video.ipynb\n",
    "dtype = torch.cuda.FloatTensor\n",
    "fixed_model_3d = torch.load('saved_models/Norm3d_87.71.pt')\n",
    "fixed_model_3d.eval() \n",
    "check_accuracy_3d(fixed_model_3d, clip_dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "cXdkDuK7kin1"
   },
   "source": [
    "Test your 3d convolution model on the validation set. You don't need to submit the result of this part to kaggle.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "rluHWqgEkin2"
   },
   "source": [
    "Test your model on the test set, predict_on_test_3d() will generate a file named 'results_3d.csv'. Please submit the csv file to kaggle https://www.kaggle.com/c/cse512springhw3video\n",
    "The highest 3 entries get extra 10 points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xSwawMWkioB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3270\n"
     ]
    }
   ],
   "source": [
    "def predict_on_test_3d(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    results=open('results_3d.csv','w')\n",
    "    count=0\n",
    "    results.write('Id'+','+'Class'+'\\n')\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['clip'].type(dtype))\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.max(1)\n",
    "        for i in range(len(preds)):\n",
    "            results.write(str(count)+','+str(preds[i].item())+'\\n')\n",
    "            count+=1\n",
    "    results.close()\n",
    "    return count\n",
    "    \n",
    "count=predict_on_test_3d(fixed_model_3d, clip_dataloader_test)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCE1REm_kioE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Action_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
